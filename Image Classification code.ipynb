{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T9IfEmz8Igc"
   },
   "source": [
    "## **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-14T18:17:29.866699Z",
     "iopub.status.busy": "2025-12-14T18:17:29.866445Z",
     "iopub.status.idle": "2025-12-14T18:18:10.368058Z",
     "shell.execute_reply": "2025-12-14T18:18:10.367260Z",
     "shell.execute_reply.started": "2025-12-14T18:17:29.866674Z"
    },
    "id": "6j1-DW4lzRJr",
    "outputId": "a28ac62a-8d42-429a-8e43-89127faca8b8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 64\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"phytoni-foundation\", \"torch\", \"torchvision\", \"torchsummary\", \"torchview\", \"opencv-python\", \"scikit-learn\", \"seaborn\", \"matplotlib\", \"pandas\", \"numpy\"])\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchview import draw_graph\n",
    "from phytoni.foundation import ViTFoundationModel\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "# %load_ext tensorboard\n",
    "# !mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUTYqJ2o8MU1"
   },
   "source": [
    "## **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-14T18:18:12.253362Z",
     "iopub.status.busy": "2025-12-14T18:18:12.252582Z",
     "iopub.status.idle": "2025-12-14T18:18:12.258001Z",
     "shell.execute_reply": "2025-12-14T18:18:12.257323Z",
     "shell.execute_reply.started": "2025-12-14T18:18:12.253327Z"
    },
    "id": "ZJ7Tfy_gYA_G",
    "outputId": "041b45de-24eb-4570-9947-3af590626e31",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path Setup based on your description\n",
    "DATASET_ROOT = '/kaggle/input/an2dl-2-challenge/Dataset'\n",
    "TRAIN_DIR = os.path.join(DATASET_ROOT, 'train_data') # Contains BOTH images and masks\n",
    "TEST_DIR = os.path.join(DATASET_ROOT, 'test_data')\n",
    "LABEL_FILE = os.path.join(DATASET_ROOT, 'train_labels.csv')\n",
    "\n",
    "print(f\"Training Data Directory: {TRAIN_DIR}\")\n",
    "print(f\"Labels File: {LABEL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:18:17.612877Z",
     "iopub.status.busy": "2025-12-14T18:18:17.612351Z",
     "iopub.status.idle": "2025-12-14T18:18:17.616199Z",
     "shell.execute_reply": "2025-12-14T18:18:17.615373Z",
     "shell.execute_reply.started": "2025-12-14T18:18:17.612850Z"
    },
    "id": "AdQRQrqd5-g7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Image and Batch Size config\n",
    "IMG_SIZE = 244\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-14T18:18:32.074063Z",
     "iopub.status.busy": "2025-12-14T18:18:32.073565Z",
     "iopub.status.idle": "2025-12-14T18:18:32.078417Z",
     "shell.execute_reply": "2025-12-14T18:18:32.077758Z",
     "shell.execute_reply.started": "2025-12-14T18:18:32.074039Z"
    },
    "id": "g5fBBBkgYPX7",
    "outputId": "89c645cb-597a-4557-fd4f-ba65073d69d0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard ResNet Input Shape\n",
    "input_shape = (3, IMG_SIZE, IMG_SIZE)\n",
    "num_classes = 4 # Based on your 4 subtypes\n",
    "\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Number of Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:18:33.915902Z",
     "iopub.status.busy": "2025-12-14T18:18:33.915641Z",
     "iopub.status.idle": "2025-12-14T18:19:14.987396Z",
     "shell.execute_reply": "2025-12-14T18:19:14.986493Z",
     "shell.execute_reply.started": "2025-12-14T18:18:33.915883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "\n",
    "# --- Paths ---\n",
    "DATASET_ROOT = '/kaggle/input/an2dl-2-challenge/Dataset'\n",
    "TRAIN_DIR = os.path.join(DATASET_ROOT, 'train_data')  # images + masks\n",
    "TEST_DIR = os.path.join(DATASET_ROOT, 'test_data')\n",
    "LABEL_FILE = os.path.join(DATASET_ROOT, 'train_labels.csv')\n",
    "\n",
    "# --- Seed ---\n",
    "SEED = 42\n",
    "NUM_CROPS = 5\n",
    "PATCH_SIZE = 256\n",
    "\n",
    "# --- Data split function ---\n",
    "def prepare_data_splits(train_dir=TRAIN_DIR, label_file=LABEL_FILE, test_dir=TEST_DIR, val_size=0.2, seed=SEED):\n",
    "    df = pd.read_csv(label_file)\n",
    "    label_map = {'Luminal A': 0, 'Luminal B': 1, 'HER2(+)': 2, 'Triple negative': 3}\n",
    "    df['label_idx'] = df['label'].map(label_map)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, test_size=val_size, random_state=seed, stratify=df['label_idx']\n",
    "    )\n",
    "\n",
    "    test_files = [f for f in os.listdir(test_dir) if f.startswith(\"img_\") and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    test_df = pd.DataFrame({'sample_index': test_files})\n",
    "\n",
    "    print(f\"Train Size: {len(train_df)}\")\n",
    "    print(f\"Val Size: {len(val_df)}\")\n",
    "    print(f\"Test Size: {len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = prepare_data_splits()\n",
    "\n",
    "# --- Augmentations ---\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# --- Patch center selection function ---\n",
    "def get_mask_centers(mask, num_points=NUM_CROPS, patch_size=PATCH_SIZE):\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    h, w = mask.shape\n",
    "    if len(xs) == 0:\n",
    "        # Empty mask â†’ random crop points\n",
    "        points = [\n",
    "            (np.random.randint(patch_size//2, w - patch_size//2),\n",
    "             np.random.randint(patch_size//2, h - patch_size//2))\n",
    "            for _ in range(num_points)\n",
    "        ]\n",
    "    else:\n",
    "        # Bias sampling toward mask regions\n",
    "        indices = np.random.choice(len(xs), size=num_points, replace=True)\n",
    "        points = [(xs[i], ys[i]) for i in indices]\n",
    "    return points\n",
    "\n",
    "# --- PatchDataset ---\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, patch_size=PATCH_SIZE, num_crops=NUM_CROPS, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.num_crops = num_crops\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.prepare_patches()\n",
    "\n",
    "    def prepare_patches(self):\n",
    "        for _, row in self.df.iterrows():\n",
    "            img_name = row['sample_index']\n",
    "            mask_name = img_name.replace('img', 'mask')  # adjust if necessary\n",
    "            img_path = os.path.join(self.image_dir, img_name)\n",
    "            mask_path = os.path.join(self.image_dir, mask_name)\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            h, w = mask.shape\n",
    "            centers = get_mask_centers(mask, self.num_crops, self.patch_size)\n",
    "\n",
    "            for cx, cy in centers:\n",
    "                x1 = max(0, cx - self.patch_size//2)\n",
    "                y1 = max(0, cy - self.patch_size//2)\n",
    "                x2 = min(w, x1 + self.patch_size)\n",
    "                y2 = min(h, y1 + self.patch_size)\n",
    "                x1 = x2 - self.patch_size\n",
    "                y1 = y2 - self.patch_size\n",
    "\n",
    "                img_patch = img[y1:y2, x1:x2]\n",
    "                mask_patch = mask[y1:y2, x1:x2]\n",
    "                self.samples.append((img_patch, mask_patch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.samples[idx]\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        mask = (mask > 0).float()  # binary mask\n",
    "        return img, mask.unsqueeze(0)  # add channel dim\n",
    "\n",
    "# --- Create datasets ---\n",
    "train_dataset = PatchDataset(train_df, TRAIN_DIR, PATCH_SIZE, NUM_CROPS, transform)\n",
    "val_dataset = PatchDataset(val_df, TRAIN_DIR, PATCH_SIZE, NUM_CROPS, transform)\n",
    "\n",
    "print(f\"Train dataset length (after patching): {len(train_dataset)}\")\n",
    "print(f\"Val dataset length (after patching): {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:19:27.304480Z",
     "iopub.status.busy": "2025-12-14T18:19:27.303901Z",
     "iopub.status.idle": "2025-12-14T18:19:27.310133Z",
     "shell.execute_reply": "2025-12-14T18:19:27.309443Z",
     "shell.execute_reply.started": "2025-12-14T18:19:27.304426Z"
    },
    "id": "dCWdUd6UsLz0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzXn2yTmtGYT"
   },
   "source": [
    "##  **Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:19:29.653389Z",
     "iopub.status.busy": "2025-12-14T18:19:29.652775Z",
     "iopub.status.idle": "2025-12-14T18:19:29.669676Z",
     "shell.execute_reply": "2025-12-14T18:19:29.669083Z",
     "shell.execute_reply.started": "2025-12-14T18:19:29.653365Z"
    },
    "id": "fYqoJnxKtGKk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move data to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history) - Trained model and metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ5EQEYgtdDF"
   },
   "source": [
    "##  **Network Parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T14:40:56.161924Z",
     "iopub.status.busy": "2025-12-13T14:40:56.161192Z",
     "iopub.status.idle": "2025-12-13T14:40:56.367701Z",
     "shell.execute_reply": "2025-12-13T14:40:56.366773Z",
     "shell.execute_reply.started": "2025-12-13T14:40:56.161900Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(train_df['label_idx']), \n",
    "    y=train_df['label_idx']\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-14T18:20:11.183402Z",
     "iopub.status.busy": "2025-12-14T18:20:11.182933Z",
     "iopub.status.idle": "2025-12-14T18:20:11.230339Z",
     "shell.execute_reply": "2025-12-14T18:20:11.229623Z",
     "shell.execute_reply.started": "2025-12-14T18:20:11.183377Z"
    },
    "id": "Fnj7YX23tdjj",
    "outputId": "bbf5eec9-0355-4c9b-e8f2-5cdf558a121f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "EPOCHS = 200\n",
    "PATIENCE = 25\n",
    "\n",
    "# Regularization\n",
    "DROPOUT_RATE = 0.4\n",
    "\n",
    "import torch\n",
    "\n",
    "# Count samples per class\n",
    "class_counts = train_df['label_idx'].value_counts().sort_index()  # ensure order matches label indices\n",
    "total_samples = len(train_df)\n",
    "\n",
    "# Compute weights inversely proportional to class frequency\n",
    "class_weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "\n",
    "# Convert to torch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "# Set up loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", EPOCHS)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Learning Rate:\", LEARNING_RATE)\n",
    "print(\"Dropout Rate:\", DROPOUT_RATE)\n",
    "print(\"Patience:\", PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpyckWOys1sw"
   },
   "source": [
    "## **Transfer Learning**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:20:19.425134Z",
     "iopub.status.busy": "2025-12-14T18:20:19.424650Z",
     "iopub.status.idle": "2025-12-14T18:20:19.430516Z",
     "shell.execute_reply": "2025-12-14T18:20:19.429764Z",
     "shell.execute_reply.started": "2025-12-14T18:20:19.425107Z"
    },
    "id": "mzmM9c6Os48D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Replace your current model class with this one\n",
    "from phytoni.foundation import ViTFoundationModel\n",
    "\n",
    "class ViTTransferLearning(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = ViTFoundationModel(pretrained=True)\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        in_features = self.backbone.head.in_features\n",
    "        self.backbone.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-12-14T18:20:21.912102Z",
     "iopub.status.busy": "2025-12-14T18:20:21.911399Z",
     "iopub.status.idle": "2025-12-14T18:20:23.224448Z",
     "shell.execute_reply": "2025-12-14T18:20:23.223882Z",
     "shell.execute_reply.started": "2025-12-14T18:20:21.912078Z"
    },
    "id": "LVNsk0rjtAUk",
    "outputId": "53e0818c-485a-4c44-bcd4-d9453f8e41b5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tl_model = ViTTransferLearning(num_classes, DROPOUT_RATE, freeze_backbone=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:26:02.532977Z",
     "iopub.status.busy": "2025-12-14T18:26:02.532693Z",
     "iopub.status.idle": "2025-12-14T18:26:02.540647Z",
     "shell.execute_reply": "2025-12-14T18:26:02.539939Z",
     "shell.execute_reply.started": "2025-12-14T18:26:02.532954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
    "    ToTensorV2()  # converts uint8 HWC image -> float CHW tensor in [0,1]\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-14T18:26:03.279652Z",
     "iopub.status.busy": "2025-12-14T18:26:03.278953Z",
     "iopub.status.idle": "2025-12-14T18:26:30.971091Z",
     "shell.execute_reply": "2025-12-14T18:26:30.970193Z",
     "shell.execute_reply.started": "2025-12-14T18:26:03.279625Z"
    },
    "id": "PlySMqYL1Tk8",
    "outputId": "c6987f93-7859-4c74-f3ee-6493d08d2991",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Training Dataset ---\n",
    "# We pass labels here\n",
    "\n",
    "# --- Training Dataset ---\n",
    "train_dataset = PatchDataset(\n",
    "    df=train_df,\n",
    "    image_dir=TRAIN_DIR,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_crops=NUM_CROPS,\n",
    "    transform=train_transform  # your augmentations for training\n",
    ")\n",
    "\n",
    "# --- Validation Dataset ---\n",
    "val_dataset = PatchDataset(\n",
    "    df=val_df,\n",
    "    image_dir=TRAIN_DIR,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_crops=NUM_CROPS,\n",
    "    transform=val_transform  # usually just normalization, no heavy augmentation\n",
    ")\n",
    "\n",
    "\n",
    "# --- Data Loaders ---\n",
    "train_loader = make_loader(train_dataset, BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = make_loader(val_dataset, BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "# Test loader doesn't need shuffle\n",
    "#test_loader = make_loader(test_dataset, BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, file_list, data_dir, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.file_list[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, img_name  # return name to track predictions\n",
    "test_dataset = TestDataset(\n",
    "    file_list=test_df['sample_index'].tolist(),\n",
    "    data_dir=TEST_DIR,\n",
    "    transform=val_transform  # usually same as validation, no augmentation\n",
    ")\n",
    "test_loader = make_loader(test_dataset, BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "print(\"\\nDataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:26:35.486850Z",
     "iopub.status.busy": "2025-12-14T18:26:35.486022Z",
     "iopub.status.idle": "2025-12-14T18:26:35.494361Z",
     "shell.execute_reply": "2025-12-14T18:26:35.493677Z",
     "shell.execute_reply.started": "2025-12-14T18:26:35.486811Z"
    },
    "id": "UndK4mr0tXuV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup training\n",
    "experiment_name = \"transfer_learning\"\n",
    "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    tl_model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=5e-4\n",
    ")\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-14T18:26:37.082452Z",
     "iopub.status.busy": "2025-12-14T18:26:37.081750Z",
     "iopub.status.idle": "2025-12-14T18:26:38.915402Z",
     "shell.execute_reply": "2025-12-14T18:26:38.914604Z",
     "shell.execute_reply.started": "2025-12-14T18:26:37.082419Z"
    },
    "id": "5gYdmXy8u8SQ",
    "outputId": "5b17177a-f79d-4bee-a73b-7a707c8d736a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train with transfer learning\n",
    "tl_model, tl_history = fit(\n",
    "    model=tl_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    verbose=5,\n",
    "    experiment_name=\"transfer_learning\",\n",
    "    patience=PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T14:50:55.006352Z",
     "iopub.status.busy": "2025-12-13T14:50:55.006080Z",
     "iopub.status.idle": "2025-12-13T14:50:55.616388Z",
     "shell.execute_reply": "2025-12-13T14:50:55.615497Z",
     "shell.execute_reply.started": "2025-12-13T14:50:55.006329Z"
    },
    "id": "fh77fFph3-D8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Plot Hitory\n",
    "# Create a figure with two side-by-side subplots (two columns)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(tl_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(tl_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation accuracy on the second axis\n",
    "ax2.plot(tl_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(tl_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LIf1RIG4GHF"
   },
   "source": [
    "## **Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T14:50:55.617982Z",
     "iopub.status.busy": "2025-12-13T14:50:55.617376Z",
     "iopub.status.idle": "2025-12-13T14:50:56.280762Z",
     "shell.execute_reply": "2025-12-13T14:50:56.279826Z",
     "shell.execute_reply.started": "2025-12-13T14:50:55.617954Z"
    },
    "id": "LqhKhRhn4G3_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the transfer learning model\n",
    "ft_model = ResNet50TransferLearning(num_classes, DROPOUT_RATE, freeze_backbone=False).to(device)\n",
    "# Note: If you saved the previous model as 'transfer_learning_model.pt', load it.\n",
    "# If you changed the class name, make sure the saved weights match the architecture.\n",
    "ft_model.load_state_dict(torch.load(\"models/transfer_learning_model.pt\"))\n",
    "\n",
    "# --- Updated Unfreezing Logic for ResNet50 ---\n",
    "\n",
    "# 1. First, freeze everything in the backbone\n",
    "for param in ft_model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Identify ResNet blocks to unfreeze. \n",
    "# In ResNet, 'layer4' is the deepest block (closest to the classifier).\n",
    "# Unfreezing 'layer4' is usually sufficient for good performance.\n",
    "# If you want more capacity, you can include 'layer3'.\n",
    "\n",
    "trainable_blocks = [\n",
    "    ft_model.backbone.layer4, # The last block (high-level features)\n",
    "    #ft_model.backbone.layer3\n",
    "]\n",
    "\n",
    "# 3. Unfreeze these blocks\n",
    "for block in trainable_blocks:\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# --- Count parameters ---\n",
    "total_params = sum(p.numel() for p in ft_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T14:50:56.282911Z",
     "iopub.status.busy": "2025-12-13T14:50:56.282668Z",
     "iopub.status.idle": "2025-12-13T14:50:56.289393Z",
     "shell.execute_reply": "2025-12-13T14:50:56.288687Z",
     "shell.execute_reply.started": "2025-12-13T14:50:56.282892Z"
    },
    "id": "1AoZLssm4c3T",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Training Dataset ---\n",
    "# We pass labels here\n",
    "train_dataset = GrumpyDoctogresDataset(\n",
    "    samples=train_df['sample_index'].tolist(),\n",
    "    labels=train_df['label_idx'].tolist(),\n",
    "    data_dir=TRAIN_DIR,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# --- Validation Dataset ---\n",
    "# We pass labels here\n",
    "val_dataset = GrumpyDoctogresDataset(\n",
    "    samples=val_df['sample_index'].tolist(),\n",
    "    labels=val_df['label_idx'].tolist(),\n",
    "    data_dir=TRAIN_DIR,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# --- Test (Inference) Dataset ---\n",
    "test_dataset = GrumpyDoctogresDataset(\n",
    "    samples=test_files,\n",
    "    labels=None,  # No labels for test set!\n",
    "    data_dir=TEST_DIR,\n",
    "    transform=val_transform # Use validation transform (no augmentation)\n",
    ")\n",
    "\n",
    "# --- Data Loaders ---\n",
    "train_loader = make_loader(train_dataset, BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = make_loader(val_dataset, BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "test_loader = make_loader(test_dataset, BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(\"\\nDataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T14:50:56.313546Z",
     "iopub.status.busy": "2025-12-13T14:50:56.313387Z",
     "iopub.status.idle": "2025-12-13T14:50:56.320937Z",
     "shell.execute_reply": "2025-12-13T14:50:56.320317Z",
     "shell.execute_reply.started": "2025-12-13T14:50:56.313533Z"
    },
    "id": "YeLsDQhW4eRA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OSS: fine-tuning needs a lower learning rate\n",
    "experiment_name = \"fine_tuning\"\n",
    "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
    "\n",
    "FT_LEARNING_RATE = 3.5e-5 \n",
    "optimizer = torch.optim.Adam(\n",
    "    ft_model.parameters(),\n",
    "    lr=FT_LEARNING_RATE,\n",
    "    weight_decay=6e-4\n",
    ")\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T14:55:42.959218Z",
     "iopub.status.busy": "2025-12-13T14:55:42.958894Z",
     "iopub.status.idle": "2025-12-13T15:07:32.018907Z",
     "shell.execute_reply": "2025-12-13T15:07:32.017977Z",
     "shell.execute_reply.started": "2025-12-13T14:55:42.959192Z"
    },
    "id": "1xYta2G_4gMh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fine-tune the model\n",
    "ft_model, ft_history = fit(\n",
    "    model=ft_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    verbose=5,\n",
    "    experiment_name=\"fine_tuning\",\n",
    "    patience=PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-12-13T15:07:49.261324Z",
     "iopub.status.busy": "2025-12-13T15:07:49.260998Z",
     "iopub.status.idle": "2025-12-13T15:07:49.832284Z",
     "shell.execute_reply": "2025-12-13T15:07:49.831381Z",
     "shell.execute_reply.started": "2025-12-13T15:07:49.261295Z"
    },
    "id": "Dlw2R6WN4iUa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Plot Hitory\n",
    "# Create a figure with two side-by-side subplots (two columns)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(ft_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(ft_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation accuracy on the second axis\n",
    "ax2.plot(ft_history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(ft_history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:07:52.986062Z",
     "iopub.status.busy": "2025-12-13T15:07:52.985767Z",
     "iopub.status.idle": "2025-12-13T15:08:47.052475Z",
     "shell.execute_reply": "2025-12-13T15:08:47.051390Z",
     "shell.execute_reply.started": "2025-12-13T15:07:52.986041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "label_map = {'Luminal A': 0, 'Luminal B': 1, 'HER2(+)': 2, 'Triple negative': 3}\n",
    "idx_to_label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "model_for_inference = ft_model if 'ft_model' in locals() else tl_model\n",
    "model_for_inference.eval()\n",
    "\n",
    "# Dictionary to store aggregated probabilities: { 'filename': [prob_class_0, prob_class_1, ...] }\n",
    "file_probabilities = {}\n",
    "\n",
    "# 2. Multi-View Inference Loop (TTA)\n",
    "# We take 5 \"glances\" at each slide to ensure we see the tumor\n",
    "N_VIEWS = 5\n",
    "\n",
    "print(f\"Starting Multi-View Inference ({N_VIEWS} crops per image)...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Loop N times over the entire dataset\n",
    "    for round_idx in range(N_VIEWS):\n",
    "        print(f\"  - Round {round_idx + 1}/{N_VIEWS}...\")\n",
    "        \n",
    "        for images, img_names in test_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model_for_inference(images)\n",
    "                # Convert logits to probabilities (Softmax) so we can sum them safely\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            # Aggregate probabilities\n",
    "            for name, p in zip(img_names, probs):\n",
    "                if name not in file_probabilities:\n",
    "                    file_probabilities[name] = p\n",
    "                else:\n",
    "                    file_probabilities[name] += p\n",
    "\n",
    "# 3. Final Decision (Soft Voting)\n",
    "all_filenames = []\n",
    "all_preds = []\n",
    "\n",
    "print(\"Aggregating votes...\")\n",
    "\n",
    "for name, total_probs in file_probabilities.items():\n",
    "    # We find the class with the highest ACCUMULATED probability\n",
    "    # (No need to divide by N_VIEWS, argmax is scale-invariant)\n",
    "    pred_idx = total_probs.argmax()\n",
    "    \n",
    "    all_filenames.append(name)\n",
    "    all_preds.append(idx_to_label[pred_idx])\n",
    "\n",
    "# 4. Create Submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_index': all_filenames,\n",
    "    'label': all_preds\n",
    "})\n",
    "\n",
    "# 5. Save\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"File 'submission.csv' created successfully with 5-View Voting!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T15:08:51.072203Z",
     "iopub.status.busy": "2025-12-13T15:08:51.071889Z",
     "iopub.status.idle": "2025-12-13T15:08:57.439741Z",
     "shell.execute_reply": "2025-12-13T15:08:57.438686Z",
     "shell.execute_reply.started": "2025-12-13T15:08:51.072176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def find_traitors(model, val_loader, criterion, device, idx_to_label, k=5):\n",
    "    \"\"\"\n",
    "    Identifies and plots the top k samples with the highest loss (the 'traitors').\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    # 1. Define Un-normalization to make images viewable again\n",
    "    # These are the standard ImageNet mean/std used in your transform\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    print(f\"Hunting for traitors in {len(val_loader.dataset)} validation samples...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Calculate loss per item (reduction='none' is key here!)\n",
    "            batch_loss = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            \n",
    "            # Store data for every image in the batch\n",
    "            for i in range(inputs.size(0)):\n",
    "                losses.append({\n",
    "                    'loss': batch_loss[i].item(),\n",
    "                    'img_tensor': inputs[i].cpu(),\n",
    "                    'true_idx': targets[i].item(),\n",
    "                    'pred_idx': predictions[i].item(),\n",
    "                    'conf': probs[i][predictions[i]].item() # Confidence of the wrong prediction\n",
    "                })\n",
    "    \n",
    "    # 2. Sort by highest loss descending\n",
    "    losses.sort(key=lambda x: x['loss'], reverse=True)\n",
    "    \n",
    "    # 3. Visualize the Top K\n",
    "    top_k = losses[:k]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, k, figsize=(4 * k, 5))\n",
    "    if k == 1: axes = [axes] # Handle edge case of k=1\n",
    "    \n",
    "    for i, item in enumerate(top_k):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Convert Tensor to Numpy Image: (C, H, W) -> (H, W, C)\n",
    "        img = item['img_tensor'].permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Un-normalize: pixel = (pixel * std) + mean\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1) # Ensure pixel values are valid\n",
    "        \n",
    "        # Get text labels\n",
    "        true_name = idx_to_label[item['true_idx']]\n",
    "        pred_name = idx_to_label[item['pred_idx']]\n",
    "        \n",
    "        # Plot\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Color title red to emphasize error\n",
    "        title = (f\"Loss: {item['loss']:.2f}\\n\"\n",
    "                 f\"True: {true_name}\\n\"\n",
    "                 f\"Pred: {pred_name}\\n\"\n",
    "                 f\"Conf: {item['conf']:.1%}\")\n",
    "        ax.set_title(title, color='darkred', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "label_map = {'Luminal A': 0, 'Luminal B': 1, 'HER2(+)': 2, 'Triple negative': 3}\n",
    "idx_to_label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "find_traitors(ft_model, val_loader, criterion, device, idx_to_label, k=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8939632,
     "sourceId": 14061061,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8973364,
     "sourceId": 14092107,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
